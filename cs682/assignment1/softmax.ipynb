{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](https://compsci682-fa18.github.io/assignments2018/assignment1/) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "from cs682.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs682/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside **cs682/classifiers/softmax.py**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.373956\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs682/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs682.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "**Your answer:** \n",
    "This is because we know that e^0=1 since our weights are very close to zero,\n",
    "Sj~0, where Sj is the score for the jth class for say an input Xi. Now consider\n",
    "an input the softmax loss function would be of the form = -log(e^0/(e^0+e^0+e^0+e^0+e^0... 10 times))\n",
    "since we have 10 classes. which reduces to -log(1/10) = -log(0.1)\n",
    "Now this will be across all inputs as well, so the mean or the total loss would be\n",
    "1/N * (N * (-log(0.1))) = -(log(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -1.419461 analytic: -1.419461, relative error: 6.170742e-09\n",
      "numerical: -2.876790 analytic: -2.876790, relative error: 2.738390e-09\n",
      "numerical: 0.003361 analytic: 0.003361, relative error: 2.064008e-06\n",
      "numerical: 0.331295 analytic: 0.331295, relative error: 1.339430e-08\n",
      "numerical: -2.514612 analytic: -2.514612, relative error: 7.223367e-10\n",
      "numerical: -0.483466 analytic: -0.483466, relative error: 5.837329e-09\n",
      "numerical: -0.656008 analytic: -0.656008, relative error: 2.667109e-09\n",
      "numerical: 0.464911 analytic: 0.464911, relative error: 3.033381e-08\n",
      "numerical: -1.816364 analytic: -1.816364, relative error: 3.636347e-09\n",
      "numerical: -1.119012 analytic: -1.119012, relative error: 4.279274e-10\n",
      "numerical: 1.681279 analytic: 1.681279, relative error: 8.274718e-09\n",
      "numerical: -0.114753 analytic: -0.114753, relative error: 2.850489e-08\n",
      "numerical: 1.100041 analytic: 1.100041, relative error: 2.398086e-09\n",
      "numerical: 0.167011 analytic: 0.167011, relative error: 9.168809e-09\n",
      "numerical: -0.954420 analytic: -0.954420, relative error: 2.549849e-09\n",
      "numerical: -1.977758 analytic: -1.977758, relative error: 5.756613e-09\n",
      "numerical: -2.014226 analytic: -2.014226, relative error: 5.055843e-09\n",
      "numerical: -0.122866 analytic: -0.122866, relative error: 1.403233e-08\n",
      "numerical: -2.278229 analytic: -2.278229, relative error: 1.690800e-09\n",
      "numerical: 0.372748 analytic: 0.372748, relative error: 2.347533e-08\n",
      "numerical: 2.947945 analytic: 2.947945, relative error: 1.359259e-09\n",
      "numerical: -1.443111 analytic: -1.443111, relative error: 1.718985e-09\n",
      "numerical: 1.312270 analytic: 1.312270, relative error: 6.533765e-10\n",
      "numerical: -1.844439 analytic: -1.844439, relative error: 2.279838e-09\n",
      "numerical: 1.319405 analytic: 1.319405, relative error: 1.400391e-09\n",
      "numerical: 0.719764 analytic: 0.719764, relative error: 9.370298e-09\n",
      "numerical: -0.102500 analytic: -0.102500, relative error: 1.883421e-08\n",
      "numerical: 3.783295 analytic: 3.783295, relative error: 2.007088e-10\n",
      "numerical: 0.409840 analytic: 0.409840, relative error: 2.186067e-08\n",
      "numerical: 1.020225 analytic: 1.020225, relative error: 1.777309e-09\n",
      "numerical: -0.157233 analytic: -0.157233, relative error: 2.569374e-08\n",
      "numerical: 0.667903 analytic: 0.667903, relative error: 2.227704e-09\n",
      "numerical: 1.732614 analytic: 1.732614, relative error: 2.432074e-09\n",
      "numerical: 2.452244 analytic: 2.452244, relative error: 1.196750e-09\n",
      "numerical: 1.904089 analytic: 1.904089, relative error: 1.454644e-09\n",
      "numerical: -1.695139 analytic: -1.695139, relative error: 4.788409e-10\n",
      "numerical: -2.036472 analytic: -2.036472, relative error: 1.580368e-09\n",
      "numerical: -1.743964 analytic: -1.743964, relative error: 3.559657e-09\n",
      "numerical: -1.852963 analytic: -1.852963, relative error: 1.264752e-10\n",
      "numerical: 1.888730 analytic: 1.888730, relative error: 1.845571e-09\n",
      "numerical: 0.088082 analytic: 0.088082, relative error: 1.254187e-08\n",
      "numerical: 1.875598 analytic: 1.875598, relative error: 8.971478e-10\n",
      "numerical: -0.309397 analytic: -0.309397, relative error: 4.020636e-08\n",
      "numerical: -0.111797 analytic: -0.111797, relative error: 3.986917e-08\n",
      "numerical: 0.517231 analytic: 0.517231, relative error: 9.548800e-09\n",
      "numerical: -1.311424 analytic: -1.311424, relative error: 3.806837e-10\n",
      "numerical: 0.095769 analytic: 0.095769, relative error: 3.547245e-08\n",
      "numerical: 1.894829 analytic: 1.894829, relative error: 1.472641e-09\n",
      "numerical: 1.288770 analytic: 1.288770, relative error: 1.602966e-09\n",
      "numerical: -0.323654 analytic: -0.323654, relative error: 2.177990e-08\n",
      "numerical: -0.496159 analytic: -0.496159, relative error: 8.331276e-09\n",
      "numerical: -0.864436 analytic: -0.864436, relative error: 5.979662e-09\n",
      "numerical: -2.179621 analytic: -2.179621, relative error: 9.204716e-10\n",
      "numerical: -0.784815 analytic: -0.784815, relative error: 7.266070e-10\n",
      "numerical: -0.222886 analytic: -0.222886, relative error: 4.547480e-09\n",
      "numerical: 0.488083 analytic: 0.488083, relative error: 1.424993e-08\n",
      "numerical: -2.204718 analytic: -2.204718, relative error: 1.284119e-10\n",
      "numerical: 0.635105 analytic: 0.635105, relative error: 5.675835e-09\n",
      "numerical: -1.631141 analytic: -1.631141, relative error: 7.148014e-10\n",
      "numerical: -2.275308 analytic: -2.275308, relative error: 4.260114e-10\n",
      "numerical: 2.177006 analytic: 2.177006, relative error: 4.379318e-10\n",
      "numerical: -2.008633 analytic: -2.008633, relative error: 1.548097e-09\n",
      "numerical: 3.927329 analytic: 3.927329, relative error: 8.398639e-10\n",
      "numerical: -0.585719 analytic: -0.585719, relative error: 2.002741e-09\n",
      "numerical: -0.710510 analytic: -0.710510, relative error: 2.335253e-09\n",
      "numerical: 2.635975 analytic: 2.635975, relative error: 8.128841e-10\n",
      "numerical: 2.969859 analytic: 2.969859, relative error: 3.329883e-10\n",
      "numerical: -1.990802 analytic: -1.990802, relative error: 2.929613e-09\n",
      "numerical: 0.075456 analytic: 0.075456, relative error: 3.712667e-08\n",
      "numerical: 1.403784 analytic: 1.403784, relative error: 3.209323e-09\n",
      "numerical: 0.776281 analytic: 0.776281, relative error: 5.774193e-09\n",
      "numerical: -0.118383 analytic: -0.118383, relative error: 4.074993e-08\n",
      "numerical: -2.765943 analytic: -2.765943, relative error: 2.256713e-09\n",
      "numerical: -0.238713 analytic: -0.238713, relative error: 1.394801e-08\n",
      "numerical: -2.484132 analytic: -2.484132, relative error: 4.254563e-09\n",
      "numerical: 0.233002 analytic: 0.233002, relative error: 8.295437e-09\n",
      "numerical: -1.566600 analytic: -1.566600, relative error: 1.184893e-09\n",
      "numerical: -0.265907 analytic: -0.265907, relative error: 5.178571e-09\n",
      "numerical: -1.941579 analytic: -1.941579, relative error: 5.899680e-10\n",
      "numerical: -3.057634 analytic: -3.057634, relative error: 1.984274e-09\n",
      "numerical: -0.646455 analytic: -0.646455, relative error: 2.625665e-09\n",
      "numerical: -2.504127 analytic: -2.504127, relative error: 4.155032e-10\n",
      "numerical: -4.754262 analytic: -4.754262, relative error: 2.765387e-10\n",
      "numerical: -1.756176 analytic: -1.756176, relative error: 3.567346e-10\n",
      "numerical: -1.000381 analytic: -1.000381, relative error: 1.466581e-09\n",
      "numerical: 3.740016 analytic: 3.740016, relative error: 1.339235e-09\n",
      "numerical: 2.868031 analytic: 2.868031, relative error: 1.006883e-09\n",
      "numerical: -0.415392 analytic: -0.415392, relative error: 9.752219e-09\n",
      "numerical: 0.474276 analytic: 0.474276, relative error: 1.545926e-08\n",
      "numerical: 4.044759 analytic: 4.044759, relative error: 2.359978e-10\n",
      "numerical: -4.996498 analytic: -4.996498, relative error: 9.762127e-11\n",
      "numerical: -0.618897 analytic: -0.618897, relative error: 1.221726e-08\n",
      "numerical: 1.097418 analytic: 1.097418, relative error: 1.938585e-09\n",
      "numerical: -0.491280 analytic: -0.491280, relative error: 1.101828e-08\n",
      "numerical: 0.592950 analytic: 0.592950, relative error: 1.885379e-09\n",
      "numerical: -4.587782 analytic: -4.587782, relative error: 5.184220e-10\n",
      "numerical: -2.745199 analytic: -2.745199, relative error: 1.524994e-09\n",
      "numerical: 0.577647 analytic: 0.577647, relative error: 7.213046e-09\n",
      "numerical: -0.887370 analytic: -0.887370, relative error: 4.048147e-09\n",
      "numerical: -1.865869 analytic: -1.865869, relative error: 3.665901e-09\n",
      "numerical: -0.737463 analytic: -0.737463, relative error: 1.016157e-08\n",
      "numerical: -0.987727 analytic: -0.987727, relative error: 5.121540e-09\n",
      "numerical: -0.796967 analytic: -0.796967, relative error: 6.926035e-09\n",
      "numerical: -1.869067 analytic: -1.869067, relative error: 6.620449e-10\n",
      "numerical: 0.399137 analytic: 0.399137, relative error: 6.578811e-09\n",
      "numerical: -1.061007 analytic: -1.061007, relative error: 2.834442e-09\n",
      "numerical: -0.444674 analytic: -0.444674, relative error: 2.166003e-08\n",
      "numerical: -0.096115 analytic: -0.096115, relative error: 1.486703e-08\n",
      "numerical: 2.081004 analytic: 2.081004, relative error: 7.396633e-10\n",
      "numerical: 1.636461 analytic: 1.636461, relative error: 5.590642e-10\n",
      "numerical: -0.515974 analytic: -0.515974, relative error: 7.133801e-09\n",
      "numerical: 2.475833 analytic: 2.475833, relative error: 3.631369e-09\n",
      "numerical: 0.843222 analytic: 0.843222, relative error: 9.865101e-09\n",
      "numerical: 2.971596 analytic: 2.971596, relative error: 1.680214e-09\n",
      "numerical: -1.124387 analytic: -1.124387, relative error: 1.205898e-09\n",
      "numerical: 1.460354 analytic: 1.460354, relative error: 1.944069e-09\n",
      "numerical: -0.530098 analytic: -0.530098, relative error: 2.563225e-09\n",
      "numerical: 0.689123 analytic: 0.689123, relative error: 5.428185e-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: -0.405927 analytic: -0.405927, relative error: 3.845142e-09\n",
      "numerical: -0.652240 analytic: -0.652240, relative error: 7.617076e-09\n",
      "numerical: 2.711792 analytic: 2.711792, relative error: 1.666931e-09\n",
      "numerical: -0.329139 analytic: -0.329139, relative error: 3.775742e-10\n",
      "numerical: 1.309509 analytic: 1.309509, relative error: 2.405489e-09\n",
      "numerical: -0.524305 analytic: -0.524305, relative error: 5.561426e-09\n",
      "numerical: -1.698269 analytic: -1.698269, relative error: 7.178069e-10\n",
      "numerical: 3.202089 analytic: 3.202089, relative error: 1.667755e-09\n",
      "numerical: -2.210111 analytic: -2.210111, relative error: 1.181429e-11\n",
      "numerical: -1.785160 analytic: -1.785160, relative error: 7.789555e-10\n",
      "numerical: -1.181443 analytic: -1.181443, relative error: 3.385599e-09\n",
      "numerical: -0.535733 analytic: -0.535732, relative error: 1.074701e-08\n",
      "numerical: -1.941302 analytic: -1.941302, relative error: 9.541387e-10\n",
      "numerical: 1.516377 analytic: 1.516377, relative error: 1.007065e-09\n",
      "numerical: 2.875089 analytic: 2.875089, relative error: 1.921034e-09\n",
      "numerical: -0.583593 analytic: -0.583593, relative error: 2.294718e-09\n",
      "numerical: -0.613077 analytic: -0.613077, relative error: 4.065606e-09\n",
      "numerical: 1.282953 analytic: 1.282953, relative error: 7.040523e-09\n",
      "numerical: -0.733799 analytic: -0.733799, relative error: 1.727021e-09\n",
      "numerical: 2.747708 analytic: 2.747708, relative error: 3.343088e-10\n",
      "numerical: -1.169371 analytic: -1.169371, relative error: 4.400934e-09\n",
      "numerical: 0.340590 analytic: 0.340590, relative error: 3.165513e-08\n",
      "numerical: -2.856654 analytic: -2.856654, relative error: 1.520999e-09\n",
      "numerical: -0.385498 analytic: -0.385498, relative error: 1.432279e-10\n",
      "numerical: -0.201804 analytic: -0.201804, relative error: 7.243998e-10\n",
      "numerical: -1.049558 analytic: -1.049558, relative error: 2.333238e-09\n",
      "numerical: -0.600887 analytic: -0.600887, relative error: 1.933452e-08\n",
      "numerical: 3.677631 analytic: 3.677631, relative error: 1.107337e-09\n",
      "numerical: 1.888752 analytic: 1.888752, relative error: 1.292500e-09\n",
      "numerical: -2.527000 analytic: -2.527000, relative error: 1.868707e-09\n",
      "numerical: 1.400012 analytic: 1.400012, relative error: 8.156436e-09\n",
      "numerical: 4.200352 analytic: 4.200352, relative error: 2.369150e-10\n",
      "numerical: -1.865350 analytic: -1.865350, relative error: 2.076284e-09\n",
      "numerical: -2.453766 analytic: -2.453766, relative error: 2.222568e-10\n",
      "numerical: 0.990314 analytic: 0.990314, relative error: 1.445683e-09\n",
      "numerical: 0.686310 analytic: 0.686310, relative error: 8.465164e-09\n",
      "numerical: -0.109937 analytic: -0.109937, relative error: 2.829312e-08\n",
      "numerical: 2.531350 analytic: 2.531350, relative error: 1.143384e-09\n",
      "numerical: -2.022683 analytic: -2.022683, relative error: 4.473608e-09\n",
      "numerical: -1.274175 analytic: -1.274175, relative error: 3.888632e-09\n",
      "numerical: 1.072015 analytic: 1.072015, relative error: 2.742647e-09\n",
      "numerical: -0.584320 analytic: -0.584320, relative error: 1.178704e-08\n",
      "numerical: 2.256796 analytic: 2.256796, relative error: 2.008302e-09\n",
      "numerical: 0.603564 analytic: 0.603563, relative error: 7.324686e-09\n",
      "numerical: 0.396948 analytic: 0.396948, relative error: 1.546494e-09\n",
      "numerical: 0.696665 analytic: 0.696665, relative error: 9.344716e-10\n",
      "numerical: -1.197807 analytic: -1.197807, relative error: 1.615698e-09\n",
      "numerical: 2.163815 analytic: 2.163815, relative error: 1.837444e-09\n",
      "numerical: -0.324956 analytic: -0.324956, relative error: 2.091412e-08\n",
      "numerical: 4.546780 analytic: 4.546780, relative error: 1.552915e-10\n",
      "numerical: 3.335030 analytic: 3.335030, relative error: 7.797257e-12\n",
      "numerical: 0.091588 analytic: 0.091588, relative error: 6.651070e-08\n",
      "numerical: 2.599087 analytic: 2.599087, relative error: 1.121876e-09\n",
      "numerical: -0.524272 analytic: -0.524272, relative error: 1.447465e-09\n",
      "numerical: -0.675301 analytic: -0.675301, relative error: 7.572331e-09\n",
      "numerical: 1.851878 analytic: 1.851878, relative error: 8.170791e-10\n",
      "numerical: 1.240836 analytic: 1.240836, relative error: 3.699385e-09\n",
      "numerical: 0.624353 analytic: 0.624353, relative error: 6.889254e-09\n",
      "numerical: -0.328561 analytic: -0.328561, relative error: 1.056227e-08\n",
      "numerical: 2.072317 analytic: 2.072317, relative error: 2.945078e-09\n",
      "numerical: -1.370528 analytic: -1.370528, relative error: 1.505576e-09\n",
      "numerical: -4.127087 analytic: -4.127087, relative error: 2.256278e-10\n",
      "numerical: 2.589342 analytic: 2.589342, relative error: 3.903618e-09\n",
      "numerical: -4.382223 analytic: -4.382223, relative error: 4.871718e-10\n",
      "numerical: 1.082307 analytic: 1.082307, relative error: 4.704710e-09\n",
      "numerical: -1.906479 analytic: -1.906479, relative error: 8.678778e-10\n",
      "numerical: 3.766943 analytic: 3.766943, relative error: 2.749380e-09\n",
      "numerical: 0.358615 analytic: 0.358615, relative error: 2.162873e-08\n",
      "numerical: -1.449296 analytic: -1.449296, relative error: 4.668911e-09\n",
      "numerical: -1.494851 analytic: -1.494851, relative error: 5.402667e-09\n",
      "numerical: 1.038168 analytic: 1.038168, relative error: 5.051998e-09\n",
      "numerical: 1.953475 analytic: 1.953475, relative error: 1.029225e-09\n",
      "numerical: 0.180519 analytic: 0.180519, relative error: 1.918896e-08\n",
      "numerical: 1.096340 analytic: 1.096340, relative error: 1.802829e-10\n",
      "numerical: -0.544517 analytic: -0.544517, relative error: 5.378151e-09\n",
      "numerical: -2.084003 analytic: -2.084003, relative error: 8.784486e-10\n",
      "numerical: 0.483393 analytic: 0.483393, relative error: 7.534853e-09\n",
      "numerical: -1.688028 analytic: -1.688028, relative error: 4.856422e-10\n",
      "numerical: -0.682030 analytic: -0.682030, relative error: 6.970100e-10\n",
      "numerical: -0.031836 analytic: -0.031836, relative error: 1.191933e-07\n",
      "numerical: -0.792316 analytic: -0.792316, relative error: 6.291598e-10\n",
      "numerical: 3.542778 analytic: 3.542778, relative error: 1.454381e-09\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs682.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 100)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.373956e+00 computed in 0.093849s\n",
      "vectorized loss: 2.373956e+00 computed in 0.117176s\n",
      "Loss difference: 0.0000000000000013322676295502\n",
      "Gradient difference: 0.000000000000314621\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs682.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %.28f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %.18f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 805.218357\n",
      "iteration 100 / 1500: loss 100.700011\n",
      "iteration 200 / 1500: loss 14.200861\n",
      "iteration 300 / 1500: loss 3.610576\n",
      "iteration 400 / 1500: loss 2.264989\n",
      "iteration 500 / 1500: loss 2.115700\n",
      "iteration 600 / 1500: loss 2.111681\n",
      "iteration 700 / 1500: loss 2.051393\n",
      "iteration 800 / 1500: loss 2.111322\n",
      "iteration 900 / 1500: loss 2.097796\n",
      "iteration 1000 / 1500: loss 2.123509\n",
      "iteration 1100 / 1500: loss 2.075804\n",
      "iteration 1200 / 1500: loss 2.013433\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "from cs682.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "regs = [2.6e4]\n",
    "lrs = [2e-7]\n",
    "for rg in regs:\n",
    "    for lr in lrs:\n",
    "        sfm = Softmax()\n",
    "        loss_hist = sfm.train(X_train, y_train, learning_rate=lr, reg=rg,\n",
    "                              num_iters=1500, verbose=True)\n",
    "        y_train_pred = sfm.predict(X_train)\n",
    "        train_acc = np.mean(y_train == y_train_pred)\n",
    "        y_val_pred = sfm.predict(X_val)\n",
    "        val_acc = np.mean(y_val == y_val_pred)\n",
    "        if best_val<val_acc:\n",
    "            best_val=val_acc\n",
    "            best_softmax=sfm\n",
    "        results[(lr,rg)] = (train_acc,val_acc)\n",
    "        print(\"reg: %d, lr %f, val_acc %f\"%(rg,lr,val_acc))\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inline Question** - *True or False*\n",
    "\n",
    "It's possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "*Your answer*: True\n",
    "\n",
    "*Your explanation*: For SVM if we add a datapoint to the training set so that the loss value (si - sj) is negative for all values of i != j where j is the the correct class, then the net loss would still be the same. However in the case of Softmax classifier, the denominator sum is the sum of all score values (elements of the X.dot(W) matrix), hence it will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
